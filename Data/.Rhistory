library(EnvStats)
s <- NULL
for (i in 1:length(threshold)) {
s[i] <- skewness(skipgrams2[count > threshold[i], ]$weight)
#hist(skipgrams[count > threshold[i], ]$n)
}
# Plot skewness vs threshold
plot(threshold, s,
xlim = c(0, 100),
ylim = range(s, na.rm = TRUE),
type = 'b',                   # Connect points with lines
pch = 19,                     # Point character
col = 'blue',                 # Point color
xlab = 'Threshold',           # X-axis label
ylab = 'Skewness',            # Y-axis label
main = 'Skewness vs Threshold',  # Title
cex.main = 1.5,               # Title size
cex.lab = 1.2,                # Axis label size
cex.axis = 1.1,               # Axis tick label size
cex = 0.5)                    # Point size
#curve(45 / sqrt(x), from = 0.1, to = 100, add = TRUE, col = 'red', lwd = 2)
# Adding grid lines
grid(nx = NULL, ny = NULL, col = 'gray', lty = 'dotted')
# Adding a horizontal line at y=0 for reference
abline(v = 20, col = 'red', lty = 2)
suppressMessages(suppressWarnings(library(igraph)))
gs <- skipgrams2 %>%
filter(weight > 20) %>%
select(word_1,word_2)%>%
graph_from_data_frame(directed = FALSE)
gs<-igraph::simplify(gs)
# Find the largest connected component
components <- igraph::clusters(gs, mode = "weak")
biggest_cluster_id <- which.max(components$csize)
vert_ids <- V(gs)[components$membership == biggest_cluster_id]
# Create subgraph of the largest component
gs2 <- igraph::induced_subgraph(gs, vert_ids)
set.seed(42)
# Applying different community detection algorithms on graph g2
kc_edge_betweenness <- cluster_edge_betweenness(gs2)
kc_fast_greedy <- cluster_fast_greedy(gs2)
kc_infomap <- cluster_infomap(gs2)
kc_label_prop <- cluster_label_prop(gs2)
kc_leading_eigen <- cluster_leading_eigen(gs2)
kc_louvain <- cluster_louvain(gs2)
kc_spinglass <- cluster_spinglass(gs2)
kc_walktrap <- cluster_walktrap(gs2)
# Calculating modularity for each community detection method
mod_edge_betweenness <- modularity(kc_edge_betweenness)
mod_fast_greedy <- modularity(kc_fast_greedy)
mod_infomap <- modularity(kc_infomap)
mod_label_prop <- modularity(kc_label_prop)
mod_leading_eigen <- modularity(kc_leading_eigen)
mod_leiden <- modularity(gs2, kc_leiden$membership)
mod_louvain <- modularity(kc_louvain)
kc_leiden <- cluster_leiden(gs2)
kc_louvain <- cluster_louvain(gs2)
kc_spinglass <- cluster_spinglass(gs2)
kc_walktrap <- cluster_walktrap(gs2)
# Calculating modularity for each community detection method
mod_edge_betweenness <- modularity(kc_edge_betweenness)
mod_fast_greedy <- modularity(kc_fast_greedy)
mod_infomap <- modularity(kc_infomap)
mod_label_prop <- modularity(kc_label_prop)
mod_leading_eigen <- modularity(kc_leading_eigen)
kc_leiden <- cluster_leiden(gs2)
2+2
# Creating a named vector of modularity values
modularity_values <- c(
Edge_Betweenness = mod_edge_betweenness,
Fast_Greedy = mod_fast_greedy,
Infomap = mod_infomap,
Label_Propagation = mod_label_prop,
Leading_Eigen = mod_leading_eigen,
Leiden = mod_leiden,
Louvain = mod_louvain,
Spinglass = mod_spinglass,
Walktrap = mod_walktrap
)
# Creating a named vector of modularity values
modularity_values <- c(
Edge_Betweenness = mod_edge_betweenness,
Fast_Greedy = mod_fast_greedy,
Infomap = mod_infomap,
Label_Propagation = mod_label_prop,
#Leading_Eigen = mod_leading_eigen,
Leiden = mod_leiden,
Louvain = mod_louvain,
Spinglass = mod_spinglass,
Walktrap = mod_walktrap
)
# Creating a named vector of modularity values
modularity_values <- c(
Edge_Betweenness = mod_edge_betweenness,
Fast_Greedy = mod_fast_greedy,
Infomap = mod_infomap,
Label_Propagation = mod_label_prop,
#Leading_Eigen = mod_leading_eigen,
#Leiden = mod_leiden,
Louvain = mod_louvain,
Spinglass = mod_spinglass,
Walktrap = mod_walktrap
)
modularity(kc_spinglass)
modularity(kc_walktrap)
modularity(kc_louvain)
modularity(gs2, kc_leiden$membership)
modularity(kc_leading_eigen)
modularity(kc_label_prop)
modularity(kc_infomap)
modularity(kc_fast_greedy)
modularity(kc_edge_betweenness)
# Display unique memberships of Louvain algorithm
unique(kc_spinglass$membership)
# Summarize the graph
summary(g2)
# Summarize the graph
summary(gs2)
# Extract word and cluster information
word <- V(gs2)$name
cluster <- kc_spinglass$membership
# Create dataframe for clusters
cluster <- cbind(word, cluster)
cluster <- as.data.frame(cluster)
# Exporting for analysis with Python and GPT
write.csv(cluster, "clusters_skip_jep.csv", row.names = FALSE, fileEncoding = "UTF-8")
load("C:/Users/Pc/Desktop/Redes/Data/.RData")
coreness(g2)
library(readr)
library(dplyr)
library(stringr)
library(tidytext)
library(writexl)
library(igraph)
library(network)
library(intergraph)
library(ergm)
library(wordcloud)
g3<-coreness(g2)
g3
g3<-igraph::induced_subgraph(g, coreness(g2))
g3
plot(g3, layout = layout_with_kk, vertex.color = adjustcolor('purple4', 0.1),
vertex.frame.color =adjustcolor('purple4', 0.5), vertex.size = ifelse(0.8*degree(g2)>1,0.8*degree(g2),1),
vertex.label = NA, edge.color=adjustcolor('gray',0.8),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
plot(g3, layout = layout_with_kk, vertex.color = adjustcolor('purple4', 0.1),
vertex.frame.color =adjustcolor('purple4', 0.5), vertex.size = ifelse(0.8*degree(g3)>1,0.8*degree(g3),1),
vertex.label = NA, edge.color=adjustcolor('gray',0.8),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
plot(g3, layout = layout_with_kk, vertex.color = adjustcolor('purple4', 0.1),
vertex.frame.color =adjustcolor('purple4', 0.5), vertex.size = ifelse(0.8*degree(g3)>1,ifelse(0.8*degree(g3)>15,15,0.8*degree(g3)),1),
vertex.label = NA, edge.color=adjustcolor('gray',0.8),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
plot(g3, layout = layout_with_kk, vertex.color = adjustcolor('purple4', 0.1),
vertex.frame.color =adjustcolor('purple4', 0.5), vertex.size = ifelse(0.8*degree(g3)>1,ifelse(0.8*degree(g3)>15,15,0.8*degree(g3)),1),
vertex.label = NA, edge.color=adjustcolor('gray',0.1),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
plot(g3, layout = layout_with_kk, vertex.color = adjustcolor('purple4', 0.4),
vertex.frame.color =adjustcolor('purple4', 0.5), vertex.size = ifelse(0.8*degree(g3)>1,ifelse(0.8*degree(g3)>15,15,0.8*degree(g3)),1),
vertex.label = NA, edge.color=adjustcolor('gray',0.1),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
plot(g3, layout = layout_with_kk, vertex.color = adjustcolor('purple4', 0.4),
vertex.frame.color =adjustcolor('purple4', 0.5), #vertex.size = ifelse(0.8*degree(g3)>1,ifelse(0.8*degree(g3)>15,15,0.8*degree(g3)),1),
vertex.label = NA, edge.color=adjustcolor('gray',0.1),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
set.seed(123)
plot(g3, vertex.color = adjustcolor('purple4', 0.4),
vertex.frame.color =adjustcolor('purple4', 0.5), #vertex.size = ifelse(0.8*degree(g3)>1,ifelse(0.8*degree(g3)>15,15,0.8*degree(g3)),1),
vertex.label = NA, edge.color=adjustcolor('gray',0.1),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
plot(g3, vertex.color = adjustcolor('purple4', 0.4),
vertex.frame.color =adjustcolor('purple4', 0.5), vertex.size = 3,
vertex.label = NA, edge.color=adjustcolor('gray',0.1),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
plot(g3, vertex.color = adjustcolor('purple4', 0.4),
vertex.frame.color =adjustcolor('purple4', 0.5), vertex.size = 5,
vertex.label = NA, edge.color=adjustcolor('gray',0.1),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
set.seed(123)
plot(g3, vertex.color = adjustcolor('purple4', 0.4),
vertex.frame.color =adjustcolor('purple4', 0.5), vertex.size = 5,
vertex.label = NA, edge.color=adjustcolor('gray',0.1),
vertex.label.color = 'purple4', vertex.label.cex = .7, vertex.label.dist = 1)
anime
anime_generes
View(anime_generes)
V(g)$anime_name<-anime_generes$Name
# Create subgraph of the largest component
g2 <- igraph::induced_subgraph(g, vert_ids)
# Display summary of the subgraph
summary(g2)
summary(g)
centralidad<-tibble(word = V(g2)$anime_name, eigen = eigen_centrality(g2, scale = T)$vector)
centralidad %>%
arrange(desc(eigen)) %>%
head(n = 10)
# Display summary of the fitted model
summary(ergm_fit)
# Display p-values for the goodness-of-fit test
ergm_gof$pval.model
# Summarize p-values for degree distribution
quantile(ergm_gof$pval.deg[,5], probs = seq(0, 1,  0.2))
# Summarize p-values for edgewise shared partner distribution
quantile(ergm_gof$pval.espart[,5], probs = seq(0, 1,  0.2))
# Summarize p-values for minimum geodesic distance distribution
quantile(ergm_gof$pval.dist[,5], probs=seq(0, 1,  0.2))
library(readr)
library(dplyr)
library(stringr)
# Set working directory
setwd("C:/Users/Pc/Desktop/Anime-Network-Project/Data")
# Read data from CSV files
animelist <- read_csv("animelist.csv")
anime <- read_csv("anime.csv")
# Display structure of the 'anime' dataframe
str(anime)
# Filter out adult content and select specific columns
anime <- anime %>%
select(MAL_ID, Name, Score, Genres) %>%
filter(!str_detect(Genres, "\\Hentai\\b")) %>%
as.data.frame()
# Display structure of the 'animelist' dataframe
str(animelist)
# Group 'animelist' by 'anime_id' and count occurrences for watching_status == 2
# 2 means the anime has already been watched
animelist_group <- animelist %>%
filter(watching_status == 2) %>%
group_by(anime_id) %>%
summarise(num = n()) %>%
arrange(desc(num)) %>%
as.data.frame()
# Left join 'anime' with 'animelist_group' and arrange by 'num'
anime <- anime %>%
left_join(animelist_group, by = c('MAL_ID' = 'anime_id')) %>%
arrange(desc(num)) %>%
as.data.frame()
# Count the occurrences of each genre
genres_list <- unlist(strsplit(anime$Genres, ", "))
genre_counts <- table(genres_list)
genre_counts <- sort(genre_counts, decreasing = TRUE)
# Print the genre counts
print(genre_counts)
# Exclude genres with fewer than 100 occurrences
generos_excluir <- rownames(which(genre_counts < 100, arr.ind = TRUE))
# Create indicator variables for genres
anime <- anime %>%
filter(!is.na(Genres) | Genres != "") %>%
as.data.frame()
# List of unique genres
genre_list <- unique(unlist(strsplit(anime$Genres, ", ")))
# Create a matrix for genre indicators
genre_matrix <- matrix(0, nrow = nrow(anime), ncol = length(genre_list), dimnames = list(NULL, genre_list))
# Populate the genre indicator matrix
for (i in 1:nrow(anime)) {
genres <- unlist(strsplit(anime$Genres[i], ", "))
genre_matrix[i, genres] <- 1
}
# Combine the genre indicator matrix with the 'anime' dataframe
anime <- cbind(anime, genre_matrix)
# Remove columns for excluded genres
anime <- anime %>%
select(-any_of(generos_excluir))
# Update the list of genres
genres_list <- setdiff(genres_list, generos_excluir)
# Initialize a vector to store standard deviations for each genre
desviaciones_estandar <- vector("numeric", length = length(genres_list))
# Calculate standard deviations for each genre
for (i in 1:length(genres_list)) {
desviaciones_estandar[i] <- sd(anime[anime[, genres_list[i]] == 1, 'Score'], na.rm = TRUE)
}
# Display structure of the 'anime' dataframe
str(anime)
# Initialize a vector to store population sizes for each genre
tamanos_poblacion <- vector("numeric", length = length(genres_list))
# Calculate population sizes for each genre
for (i in 1:length(genres_list)-1) {
tamanos_poblacion[i] <- length(anime[anime[, genres_list[i]] == 1, 'Score'])
}
# Display standard deviations and population sizes
desviaciones_estandar
tamanos_poblacion
# Function to calculate sample size
tam.muestra <- function(alfa, epsilon, s, N = Inf) {
za2 <- qnorm(1 - alfa / 2)
if (N == Inf) {
n <- (s * za2 / epsilon)^2
} else {
n <- N * ((za2 * s)^2) / ((N - 1) * epsilon^2 + (za2 * s)^2)
}
return(ceiling(n))
}
# Define parameters for sample size calculation
alfa <- 0.15 # Confidence level
epsilon <- 0.2 # Margin of error
# Initialize a vector to store sample sizes for each genre
tamanos_muestra <- vector("numeric", length = length(genres_list))
# Calculate sample sizes for each genre
for (i in 1:length(genres_list)) {
N <- tamanos_poblacion[i]
s <- desviaciones_estandar[i]
tamanos_muestra[i] <- tam.muestra(alfa, epsilon, s, N)
}
# Display the sample sizes
tamanos_muestra <- tamanos_muestra
sum(tamanos_muestra)
sum(tamanos_poblacion)
# Initialize a vector to store sampled data
muestras <- vector()
# Perform random sampling for each genre
for (i in 1:(length(genres_list) - 1)) {
# Set random seed for reproducibility
set.seed(123)
muestras <- c(muestras, sample(anime[anime[, genres_list[i]] == 1, 'MAL_ID'], size = tamanos_muestra[i], replace = FALSE))
}
# Remove duplicate samples and convert to dataframe
muestras <- unique(muestras)
length(muestras)
library(readr)
library(dplyr)
library(stringr)
library(tidytext)
library(writexl)
library(igraph)
library(network)
library(intergraph)
library(ergm)
library(wordcloud)
# Set working directory
setwd("C:/Users/Pc/Desktop/Anime-Network-Project/Data")
# Read data from CSV files
animelist <- read_csv("animelist.csv")
anime <- read_csv("anime.csv")
anime_with_synopsis <- read_csv("anime_with_synopsis.csv")
# Display structure of 'anime_with_synopsis' dataframe
str(anime_with_synopsis)
# Filter 'anime_with_synopsis' dataframe
anime_with_synopsis <- anime_with_synopsis %>%
filter(!str_detect(Genres, "\\Hentai\\b")) %>%  # Exclude genres containing 'Hentai'
filter(!is.na(anime_with_synopsis$sypnopsis)) %>%  # Remove rows with missing synopsis
filter(!str_detect(sypnopsis, 'No synopsis information has been added to this title. Help improve our database by adding a synopsis here.')) %>%  # Remove rows with default placeholder text
as.data.frame()
# Clean the 'sypnopsis' text data
anime_with_synopsis <- within(anime_with_synopsis, {
synopsis <- str_to_lower(sypnopsis)  # Convert to lowercase
synopsis <- str_replace_all(synopsis, "[^a-z ]", "")  # Remove non-alphabetic characters
synopsis <- str_replace_all(synopsis, "usic", " music ")  # Replace 'usic' with 'music'
synopsis <- str_squish(synopsis)  # Remove extra whitespaces
})
# Concatenate all synopsis into a single string separated by '|'
synopsis <- paste(anime_with_synopsis$synopsis, collapse = " | ")
# Remove names attribute
names(synopsis) <- NULL
# Convert to tibble
synopsis <- tibble(line = 1:length(synopsis), text = synopsis)
# Tokenize text by splitting on whitespace
synopsis <- synopsis %>%
unnest_tokens(input = text, output = word, token = "regex", pattern = "\\s+")
# Join with stop words and remove them
synopsis <- synopsis %>%
anti_join(x = ., y = stop_words)
# Join with stop words and remove them
synopsis2 <- synopsis %>%
filter(!str_detect(word, "\\bsource\\b")) %>%
filter(!str_detect(word, "^\\||\\|$")) %>%
anti_join(x = ., y = stop_words)
synopsis2  %>%
count(word, sort = TRUE) %>%
head(n = 10)
set.seed(123)
synopsis2 %>%
count(word, sort = TRUE) %>%
with(wordcloud(words = word, freq = n, max.words =300, colors = 'purple2'))
# Extract tokens
tokens <- synopsis$word
# Create bigrams from tokens and avoid multiple edges
bigrams <- sapply(1:(length(tokens) - 1), function(i) {
words <- sort(c(tokens[i], tokens[i + 1]))
paste(words, collapse = " ")
})
# Convert bigrams to a vector
bigrams <- as.vector(bigrams)
# Convert bigrams to dataframe
bigrams <- as.data.frame(bigrams)
# Filter out undesired bigrams
bigrams <- bigrams %>%
filter(!str_detect(bigrams, "^\\||\\|$")) %>%
filter(!str_detect(bigrams, "\\bsource\\b")) %>%
as.data.frame()
bigrams<-bigrams$bigrams
bigrams<- str_split_fixed(bigrams, " ", 2)
word_1 <- bigrams[, 1]
word_2 <- bigrams[, 2]
bigrams2 <- cbind(word_1, word_2)
bigrams2 <- as.data.frame(bigrams2)
bigrams2<-bigrams2%>%
filter(!word_1 %in% stop_words$word & !word_2 %in% stop_words$word)%>%
filter(word_1 != word_2)%>%
count(word_1, word_2, sort = TRUE) %>%
rename(weight = n)
# Calculate skewness for different thresholds
threshold <- unique(bigrams2$weight)
count <- bigrams2$weight
library(EnvStats)
s <- NULL
for (i in 1:length(threshold)) {
s[i] <- skewness(bigrams2[count > threshold[i], ]$weight)
#hist(bigrams[count > threshold[i], ]$n)
}
# Plot skewness vs threshold
plot(threshold, s,
xlim = c(0, 100),
ylim = range(s, na.rm = TRUE),
type = 'b',                   # Connect points with lines
pch = 19,                     # Point character
col = 'blue',                 # Point color
xlab = 'Threshold',           # X-axis label
ylab = 'Skewness',            # Y-axis label
main = '',  # Title
cex.main = 1.5,               # Title size
cex.lab = 1.2,                # Axis label size
cex.axis = 1.1,               # Axis tick label size
cex = 0.5)                    # Point size
#curve(45 / sqrt(x), from = 0.1, to = 100, add = TRUE, col = 'red', lwd = 2)
# Adding grid lines
grid(nx = NULL, ny = NULL, col = 'gray', lty = 'dotted')
# Adding a horizontal line at y=0 for reference
abline(v = 20, col = 'red', lty = 2)
head(bigrams2,10)
g <- bigrams2%>%
filter(weight > 20) %>%
select(word_1,word_2)%>%
graph_from_data_frame(directed = FALSE)
g<-igraph::simplify(g)
# Find the largest connected component
components <- igraph::clusters(g, mode = "weak")
biggest_cluster_id <- which.max(components$csize)
vert_ids <- V(g)[components$membership == biggest_cluster_id]
# Create subgraph of the largest component
g2 <- igraph::induced_subgraph(g, vert_ids)
set.seed(42)
# Applying different community detection algorithms on graph g2
kc_edge_betweenness <- cluster_edge_betweenness(g2)
kc_fast_greedy <- cluster_fast_greedy(g2)
kc_infomap <- cluster_infomap(g2)
kc_label_prop <- cluster_label_prop(g2)
kc_leading_eigen <- cluster_leading_eigen(g2)
kc_leiden <- cluster_leiden(g2)
kc_louvain <- cluster_louvain(g2)
kc_spinglass <- cluster_spinglass(g2)
kc_walktrap <- cluster_walktrap(g2)
# Calculating modularity for each community detection method
mod_edge_betweenness <- modularity(kc_edge_betweenness)
mod_fast_greedy <- modularity(kc_fast_greedy)
mod_infomap <- modularity(kc_infomap)
mod_label_prop <- modularity(kc_label_prop)
mod_leading_eigen <- modularity(kc_leading_eigen)
mod_leiden <- modularity(g2, kc_leiden$membership)
mod_louvain <- modularity(kc_louvain)
mod_spinglass <- modularity(kc_spinglass)
mod_walktrap <- modularity(kc_walktrap)
# Creating a named vector of modularity values
modularity_values <- c(
Edge_Betweenness = mod_edge_betweenness,
Fast_Greedy = mod_fast_greedy,
Infomap = mod_infomap,
Label_Propagation = mod_label_prop,
Leading_Eigen = mod_leading_eigen,
Leiden = mod_leiden,
Louvain = mod_louvain,
Spinglass = mod_spinglass,
Walktrap = mod_walktrap
)
# Finding the method with the highest modularity
best_method <- names(which.max(modularity_values))
best_modularity <- max(modularity_values)
# Printing the method with the highest modularity
cat("Based on the modularity values, the method with the highest modularity is", best_method, "\n")
cat("Modularity value:", best_modularity, "\n")
k<-kc_spinglass
# Display unique memberships of Louvain algorithm
max(k$membership)
cols <- c(brewer.pal(9,"Set1")[1:9],brewer.pal(8,"Set2")[1:7],brewer.pal(8,"Set2")[1:7],brewer.pal(12,"Set3")[1:3])
plot(g2, layout = layout_with_kk, vertex.color = adjustcolor(cols[k$membership], 0.6),
vertex.frame.color =adjustcolor(cols[kc_spinglass$membership],1), vertex.size = 3,
vertex.label = NA, edge.color=adjustcolor('gray',0.8),
vertex.label.color = 'black', vertex.label.cex = .7, vertex.label.dist = 1)
set.seed(123)
plot(g2, layout = layout_nicely, vertex.color = adjustcolor(cols[k$membership], 0.6),
vertex.frame.color =adjustcolor(cols[kc_spinglass$membership],1), vertex.size = 3,
vertex.label = NA, edge.color=adjustcolor('gray',0.8),
vertex.label.color = 'black', vertex.label.cex = .7, vertex.label.dist = 1)
eigen_centrality(g2)$vector
tab <- cbind(c("Dist. media","Grado media","Grado desviación","Número clan","Densidad","Transitividad","Asortatividad"),
round(c(mean_distance(g2), mean(degree(g2)), sd(degree(g2)), clique.number(g2), edge_density(g2),
transitivity(g2), assortativity_degree(g2)),4)
)
tab
# Summarize the graph
summary(g2)
# Extract word and cluster information
word <- V(g2)$name
cluster <- k$membership
eigen_centrality(g2)$vector
V(g2)$name
eigen_word<-eigen_centrality(g2)$vector
# Create dataframe for clusters
cluster <- cbind(word, cluster,eigen_word)
cluster <- as.data.frame(cluster)
# Exporting for analysis with Python and GPT
write.csv(cluster, "clusters.csv", row.names = FALSE)
write.csv(bigrams2, "bigrams.csv", row.names = FALSE)
